# VQA_MODEL

The task of visual question answering (VQA) is to generate an answer for a 
question according to the content of an image being asked. In VQA, an algorithm 
needs to answer text-based questions about images. In this process, the critical 
problems of effectively embedding the question feature and image feature as well 
as transforming the features to the prediction of answer are still faithfully 
unresolved.  
Existing models analyze the input images at a typically small resolution, often 
leading to discarding valuable fine-grained details. To overcome this limitation, 
in this work reinforcement learning based approach is proposed  that works by 
applying a series of transformation operations on the images (translation) in order 
to facilitate answering the question at hand. This allows for performing fine
grained analysis, by only focusing on parts of the image that are relevant to the 
question in hand. 
Visual Question Answering(VQA 2.0 ) dataset is used for the purpose of training 
and testing this proposed model.  
